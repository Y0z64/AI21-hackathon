{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKr4pcE5V967"
      },
      "source": [
        "#**STORIES**\n",
        "###Made by: (Y0z64)[https://github.com/Y0z64]\n",
        "\n",
        "##Instructions\n",
        "- Run the first cell in **Requirements** and wait for it to display \"Requirements correctly installed\"\n",
        "- Run the second cell and wait for it to load\n",
        "- Run the last cell under **Start app**\n",
        "- When the app finishes loading CNTR+click the public link or use locally in colab\n",
        "\n",
        "##Getting started\n",
        "- Write start in the input and wait for a prompt to appear\n",
        "- The space under the chat shows recommended actions to take, to make an action just write it in the input box\n",
        "- Under the recommended action the image for the actual propmt will be displayed\n",
        "\n",
        "##Knwon issues\n",
        "- If after sometime the image generation gets too slow, close the window and re-run the last cell\n",
        "- Sometimes the prompt generation can get repetitive or incoherent, those are quirks of the generation, if it gets too bad close window and re-run last cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4etRKKxmtdT",
        "outputId": "04f91cf6-3bf5-4eb8-af84-db4f0387bf6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gpu running correctly\n"
          ]
        }
      ],
      "source": [
        "#@markdown ###Check if there is conected gpu\n",
        "from IPython.display import clear_output\n",
        "\n",
        "try:\n",
        "  !nvidia-smi\n",
        "  clear_output()\n",
        "  print(\"Gpu running correctly\")\n",
        "except:\n",
        "  print(\"No gpu connected\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrZZ09WUbjU3"
      },
      "source": [
        "##**Requirements**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgK7KbslU1uU",
        "outputId": "18e21fed-22d4-4004-aeaa-aea154e9fe05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Installed interface requirements\n",
            "\n",
            "[Errno 2] No such file or directory: '/diffusers'\n",
            "/content\n",
            "Cloning into 'diffusers'...\n",
            "remote: Enumerating objects: 17868, done.\u001b[K\n",
            "remote: Counting objects: 100% (305/305), done.\u001b[K\n",
            "remote: Compressing objects: 100% (214/214), done.\u001b[K\n",
            "remote: Total 17868 (delta 157), reused 164 (delta 77), pack-reused 17563\u001b[K\n",
            "Receiving objects: 100% (17868/17868), 11.92 MiB | 27.99 MiB/s, done.\n",
            "Resolving deltas: 100% (12399/12399), done.\n"
          ]
        }
      ],
      "source": [
        "!pip install wget\n",
        "import os\n",
        "import wget\n",
        "from IPython.display import clear_output\n",
        "\n",
        "#install diffusers for txt2image generation\n",
        "!pip install --upgrade diffusers[torch] torch\n",
        "clear_output()\n",
        "print(\"\\nSuccessfully installed diffusers[torch]\\n\")\n",
        "\n",
        "#install diffusers requirements\n",
        "!pip install --upgrade diffusers transformers accelerate safetensors OmegaConf\n",
        "clear_output()\n",
        "print(\"\\nInstalled txt2image requirements\\n\")\n",
        "\n",
        "#install interface requirements\n",
        "!pip install gradio requests\n",
        "clear_output()\n",
        "print(\"\\nInstalled interface requirements\\n\")\n",
        "\n",
        "#Install diffusers\n",
        "if not os.path.exists(\"/content/diffusers\"):\n",
        "  !mkdir /content/diffusers\n",
        "  %cd /diffusers\n",
        "  !git clone https://github.com/huggingface/diffusers\n",
        "  !git config --global user.email \"you@example.com\"\n",
        "  %cd /content/diffusers\n",
        "  !git checkout 8a3f0c1f7178f4a3d5a5b21ae8c2906f473e240d\n",
        "  clear_output()\n",
        "  print(\"\\nSuccesfully installed Diffusers\\n\")\n",
        "  %cd /content\n",
        "else:\n",
        "  print(\"\\nDiffusers alredy installed\\n\")\n",
        "\n",
        "#Clone project repository\n",
        "if not os.path.exists(\"/content/adventure\"):\n",
        "  !mkdir /content/adventure\n",
        "  %cd /content/adventure\n",
        "  !git clone https://github.com/Y0z64/Mythmaker\n",
        "  clear_output()\n",
        "  print(\"\\nCorrectly cloned repository\")\n",
        "  %cd /content\n",
        "else:\n",
        "  print(\"\\nRepository already cloned\\n\")\n",
        "\n",
        "#Install model\n",
        "if not os.path.exists(\"/content/stable_diffusion\"):\n",
        "  !mkdir /content/stable_diffusion\n",
        "  %cd /content/stable_diffusion\n",
        "  !wget -O protogen_x34.safetensors \"https://civitai.com/api/download/models/4048?type=Model&format=SafeTensor\"\n",
        "  clear_output()\n",
        "  print(\"\\nCorrectly installed model\\n\")\n",
        "else:\n",
        "  clear_output()\n",
        "\n",
        "clear_output()\n",
        "print(\"Requirements correctly installed\")\n",
        "%cd /content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e4QI6xeg0Tf"
      },
      "source": [
        "###**Convert model to diffusers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6yfQojVg65k",
        "outputId": "9a148a90-19a9-430b-97f6-0bd94652a7e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model already converted\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from IPython.display import clear_output\n",
        "import sys\n",
        "\n",
        "sys.path.append(\"/usr/local/lib/python3.8/dist-packages\")\n",
        "\n",
        "\n",
        "if not os.path.exists(\"/content/stable_diffusion/model\"):\n",
        "  %cd /content/stable_diffusion\n",
        "  !mkdir model\n",
        "  !python /content/diffusers/scripts/convert_original_stable_diffusion_to_diffusers.py --from_safetensors --checkpoint_path /content/stable_diffusion/protogen_x34.safetensors --dump_path /content/stable_diffusion/model\n",
        "  \n",
        "  print(\"Correctly converted model to Diffusers\\n\")\n",
        "  if not os.path.exists(\"/content/stable_diffusion/model\"):\n",
        "    !rm /content/stable_diffusion/protogen_x34.safetensors\n",
        "\n",
        "else:\n",
        "  clear_output()\n",
        "  print(\"Model already converted\")\n",
        "\n",
        "%cd /content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0v1qau4l44U"
      },
      "source": [
        "##**Start app**\n",
        "When the link appears click it to run it in your browser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "m8HRlTWyl_14",
        "outputId": "02e19097-c6bc-4d4d-c6d3-7b029254396a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "Custom app was imported correctly\n"
          ]
        },
        {
          "ename": "OSError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-e57fa947931e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#Initialize image generator with custom pretrained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mpipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStableDiffusionPipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/stable_diffusion/model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mpipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/diffusers/pipeline_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mcached_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcached_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0;31m# 2. Load the pipeline class, if using custom module then load it from the hub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/diffusers/configuration_utils.py\u001b[0m in \u001b[0;36mload_config\u001b[0;34m(cls, pretrained_model_name_or_path, return_unused_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 \u001b[0mconfig_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 raise EnvironmentError(\n\u001b[0m\u001b[1;32m    321\u001b[0m                     \u001b[0;34mf\"Error no file named {cls.config_name} found in directory {pretrained_model_name_or_path}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 )\n",
            "\u001b[0;31mOSError\u001b[0m: Error no file named model_index.json found in directory /content/stable_diffusion/model."
          ]
        }
      ],
      "source": [
        "\n",
        "#add app to system path\n",
        "import sys\n",
        "\n",
        "#!cat /content/adventure/AI21-hackathon/adventures.py\n",
        "sys.path.append(\"/content/adventure/AI21-hackathon\")\n",
        "!cd /content/adventure/AI21-hackathon\n",
        "!pwd\n",
        "from adventures import *\n",
        "print(\"Custom app was imported correctly\")\n",
        "\n",
        "\n",
        "import torch\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from IPython.display import Image\n",
        "from time import sleep\n",
        "import gradio as gr\n",
        "from IPython.display import clear_output\n",
        "#import app\n",
        "\n",
        "#Initialize image generator with custom pretrained model\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\"/content/stable_diffusion/model\", torch_dtype=torch.float16)\n",
        "pipe = pipe.to(\"cuda\")\n",
        "generator = torch.Generator(\"cuda\")\n",
        "\n",
        "#Get API KEY\n",
        "api_key = input(\"Input your API key: \")\n",
        "\n",
        "#start the adventure\n",
        "action_list, default_prpt = start_chat(fantasy_preset, start_1, api_key)\n",
        "\n",
        "def display_chatbot(input_message, Temperature, hystory=[]):\n",
        "    #call the chatbot\n",
        "    history, history, actions, message = chatbot_requests(input_message, action_list, default_prpt, Temperature, api_key, hystory)\n",
        "\n",
        "    #generate image\n",
        "    generation_prompt = generate_prompt(message, api_key)\n",
        "    image = pipe(generation_prompt, guidance_scale=7.5, num_inference_steps=15, generator=generator).images[0]\n",
        "\n",
        "    return history, history, actions, image\n",
        "\n",
        "#end adventure\n",
        "def clear_save():\n",
        "    global save\n",
        "    save = []\n",
        "\n",
        "#Interface design\n",
        "demo = gr.Interface(fn=display_chatbot,\n",
        "        inputs=[\"text\", gr.Slider(0, 1), \"state\"],\n",
        "        outputs=[\"chatbot\", \"state\", \"text\", \"image\"])\n",
        "\n",
        "#launch interface\n",
        "demo.launch(debug=True, share=True)\n",
        "\n",
        "#close everyhting\n",
        "clear_output()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}